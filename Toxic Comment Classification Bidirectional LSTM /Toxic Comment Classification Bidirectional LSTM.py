# -*- coding: utf-8 -*-
"""06_DL_Deep_Learning_For_Text_Data_(LSTM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16OFV9Am92iAowOqKAjKWSod4jZH_3YA-

## Toxic Comment Classification Bidirectional LSTM 

---



In this notebook I will be using Bidrectional LSTM for Toxic Comment Classification. This notebook follows the same template as 1-D CNN and LSTM but I will be using BLSTM instead.  

### The Data 

The data consists of a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are
 - toxic
 - severe_toxic
 - obscene
 - threat 
 - insult
 - identity_hate 
 
The levels of toxicity are multiple labels that we need to classify. The link to data is given below 

https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data

Here, I will be using pre-trained Glove word embeddings from Stanford which can be downloaded from the following link. 

http://nlp.stanford.edu/data/glove.6B.zip
"""

# Mount drive 
from google.colab import drive
drive.mount('/content/gdrive')

path = "//content//gdrive//My Drive//AI School Class//"
import os
os.path.isdir(path)

"""### Basic Imports """

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import LSTM,Embedding,Bidirectional
from keras.models import Model
from sklearn.metrics import roc_auc_score

"""### Constants """

MAX_SEQUENCE_LENGTH = 100
MAX_VOCAB_SIZE = 20000
EMBEDDING_DIM = 100 # pre-trained embeddings only come in specific sizes, so we can only use 50,100,200 or 300
VALIDATION_SPLIT = 0.2
BATCH_SIZE = 128
EPOCHS = 5

"""### Loading Word Embeddings """

word2vec = {}
EmbeddingPath = path + "Word Embedding Data//"
with open(os.path.join(EmbeddingPath + 'glove.6B.100d.txt'),encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vec = np.asarray(values[1:], dtype='float32')
        word2vec[word] = vec
print('Found {} word vectors.'.format(len(word2vec)))

print(word2vec['learn'])

dist = np.linalg.norm(word2vec['mother'] - word2vec['father'])
dist

dist = np.linalg.norm(word2vec['mother'] - word2vec['child'])
dist

dist = np.linalg.norm(word2vec['study'] - word2vec['learn'])
dist

dist = np.linalg.norm(word2vec['study'] - word2vec['studied'])
dist

"""### Loading Data"""

DataPath = path + "Toxic Data//"
data_train = pd.read_csv(DataPath + 'train.csv')

data_train.head()

data_train.shape

data_train.info()

"""### Seprating comments from the data frame"""

comments = data_train['comment_text'].fillna('Dummy_Value').values

comments.shape

type(comments)

"""### Creating labes for the multi-label classification """

labels = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

list(data_train.columns)[1:]

targets = data_train[labels].values

# targets shape : (Num of Samples,Num of target labels)
targets.shape

"""### Printing out some information about the comments"""

print('Max Sentence Length : ', max(len(sentence) for sentence in  comments))

print('Min Sentence Lenth :', min(len(sentence) for sentence in comments))

sorted_comments = sorted(len(sentensce) for sentensce in comments)
print('Median Sentence Lenth :', sorted_comments[len(sorted_comments)//2])

"""### Tokenizing """

tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)

tokenizer.fit_on_texts(comments)

sequences = tokenizer.texts_to_sequences(comments)

print(sequences[0])

len(sequences)

comments[0]

print("Max sequence length:", max(len(s) for s in sequences))

print("Max sequence length:", min(len(s) for s in sequences))

sorted_seq = sorted(len(s) for s in sequences)
print("Median sequence length:", sorted_seq[len(sorted_seq) // 2])

"""###  Word to Integer mapping"""

word2idx = tokenizer.word_index

print('Number of Unique tokens are : {}'.format(len(word2idx)) ) 
# We actually have more words than the MAX_VOCAB_SIZE!!

MAX_VOCAB_SIZE

MAX_SEQUENCE_LENGTH

"""### Padding """

pad_data = pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)

pad_data

len(pad_data)

# Note that the 0 axis is same as comments, just added a new padding dimension which is equal to MAX_SEQUENCE_LENGTH . 
# Padded tensor shape : (Num of Samples,MAX_SEQUENCE_LENGTH)

print('Shape of Padded Tensor: {}'.format(pad_data.shape))

"""### Prepare embedding matrix"""

# This truncates the word embeddings to MAX_VOCAB_SIZE, +1 is to take care of a little indexing problem.

# In Keras 0 is reserved just for padding, so the indexing starts at 1. 

# Since, python index starts at 0, that +1 takes care of any indexing miss matching. 

num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)

num_words

embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))

embedding_matrix

embedding_matrix.shape
# We have successfuly trucnated and created an embedding matrix of dimension (MAX_VOCAB_SIZE,EMBEDDING_DIM )
# Now we have to index it using the words from the pre-trained word2vec to create pre-trained embedding matrix

for word,i in word2idx.items():
    if i < MAX_VOCAB_SIZE:
        embedding_vector = word2vec.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all zeros.
            embedding_matrix[i] = embedding_vector

embedding_matrix[2]

"""### Loading pre-trained word embeddings into an Embedding layer"""

embedding_layer = Embedding(input_dim=num_words,output_dim=EMBEDDING_DIM, weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,trainable=False)

"""### Buiding the Model """

input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))

x = embedding_layer(input_)
x = Bidirectional(LSTM(20, return_sequences=True))(x)
x = GlobalMaxPooling1D()(x) # Grabs the maximum value from the entire seqence 
output = Dense(len(labels), activation='sigmoid')(x)

model = Model(input_,output)

print(model.summary())

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""### Training """

trained_model = model.fit(pad_data,targets,batch_size=BATCH_SIZE,epochs=EPOCHS,validation_split=VALIDATION_SPLIT)

# save
model.save(path + 'Transfer Learning Models/Toxic_LSTM.h5')

# load
from keras.models import load_model, model_from_json
model = load_model(path + 'Transfer Learning Models/Toxic_LSTM.h5')

"""### Plotting Accuracy"""

plt.plot(trained_model.history['accuracy'])
plt.plot(trained_model.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'Validation'], loc='upper left')

"""#### Not a lot of change here

### Plotting Loss
"""

plt.plot(trained_model.history['loss'])
plt.plot(trained_model.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')

"""#### Same here as well

### Plotting mean AUC over each label
"""

pred = model.predict(pad_data)

auc_accuracy = []
for i in range(len(labels)):
    auc = roc_auc_score(targets[:,i], pred[:,i])
    auc_accuracy.append(auc)
print('Mean AUC score for all labels:',np.mean(auc_accuracy))